{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k6lKjESWQxmu"
   },
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wHwfQHdhQ4uv",
    "outputId": "e5c7c963-8b50-4ff7-9827-8a09bd72931b"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install transformers textblob\n",
    "!pip install nltk\n",
    "!pip install accelerate -U\n",
    "!pip install torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
      "[nltk_data] Downloading package wordnet to /home/ec2-user/nltk_data...\n",
      "[nltk_data] Downloading package punkt to /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "from textblob import TextBlob\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "A75rtgCdUd4R"
   },
   "outputs": [],
   "source": [
    "dataset_path = \"dataset - modified - equal - v3.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "nw0VtAEDUkP5",
    "outputId": "fc4a25d0-171d-4e68-c11c-e69bddd916cf"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hate_speech_count</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>!!!!!!!!!!!!!!!!!!\"@T_Madison_x: The shit just...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>\" @rhythmixx_ :hobbies include: fighting Maria...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>\" bitch get up off me \"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>\" bitch who do you love \"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>\" these hoes like niggas that spend money not ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10041</th>\n",
       "      <td>0</td>\n",
       "      <td>&amp;#8220;@chloeonvine: Are. You. Kidding. Me. Ri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10042</th>\n",
       "      <td>0</td>\n",
       "      <td>&amp;#8220;@chloeonvine: im just a sarcastic lil b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10043</th>\n",
       "      <td>0</td>\n",
       "      <td>&amp;#8220;@chrisbrown: These hoes ain't loyal [i ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10044</th>\n",
       "      <td>0</td>\n",
       "      <td>&amp;#8220;@clzcdxx: im just a bitch ass elf&amp;#8221...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10045</th>\n",
       "      <td>0</td>\n",
       "      <td>&amp;#8220;@cmencarini: I dont trust girls&amp;#8221; ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10046 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       hate_speech_count                                              tweet\n",
       "0                      1  !!!!!!!!!!!!!!!!!!\"@T_Madison_x: The shit just...\n",
       "1                      1  \" @rhythmixx_ :hobbies include: fighting Maria...\n",
       "2                      1                            \" bitch get up off me \"\n",
       "3                      1                          \" bitch who do you love \"\n",
       "4                      1  \" these hoes like niggas that spend money not ...\n",
       "...                  ...                                                ...\n",
       "10041                  0  &#8220;@chloeonvine: Are. You. Kidding. Me. Ri...\n",
       "10042                  0  &#8220;@chloeonvine: im just a sarcastic lil b...\n",
       "10043                  0  &#8220;@chrisbrown: These hoes ain't loyal [i ...\n",
       "10044                  0  &#8220;@clzcdxx: im just a bitch ass elf&#8221...\n",
       "10045                  0  &#8220;@cmencarini: I dont trust girls&#8221; ...\n",
       "\n",
       "[10046 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(dataset_path)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hfCaRsV4Wl3x",
    "tags": []
   },
   "source": [
    "### **Dataset-preprocessing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "FThEj3wMWuaw"
   },
   "outputs": [],
   "source": [
    "#Data Cleaning\n",
    "def remove_unwanted_text(content):\n",
    "    '''\n",
    "    Removes unwanted text from content using regex\n",
    "    Input\n",
    "    content: A string\n",
    "    Output\n",
    "    final: the final parsed string\n",
    "    '''\n",
    "    #\\s is white space, ^ is a negation of the charecter set [], + means followed by any charecter specified\n",
    "    handle = re.sub('@[^\\s]+', '', content)\n",
    "    link = re.sub('http[^\\s]+', '', handle)\n",
    "    link = re.sub('www[^\\s]+', '', link)\n",
    "    ht = re.sub('#[^\\s]+', '', link)\n",
    "    final = re.sub('&[^\\s]+', '', ht)\n",
    "    return final\n",
    "\n",
    "def remove_punctuations(words):\n",
    "    '''\n",
    "    Input\n",
    "    words: A list of words to be processed\n",
    "    Output\n",
    "    returns a list of words that punctuations and numbers have been removed.\n",
    "    '''\n",
    "    new_words = []\n",
    "    for w in words:\n",
    "        l = re.sub('[^A-Za-z ]+', '', w)\n",
    "        if l != '':\n",
    "            new_words.append(l)\n",
    "    return new_words\n",
    "\n",
    "def remove_stop_words(words):\n",
    "    '''\n",
    "    Input\n",
    "    words: Words to be processed\n",
    "    Output\n",
    "    returns a list of words without English stopwords\n",
    "    '''\n",
    "    sw = stopwords.words(\"english\") # English Stop Words\n",
    "    #Make sure that the stopwords also dont have punctioations\n",
    "    sw = remove_punctuations(sw)\n",
    "    return [w for w in words if w.lower() not in sw]\n",
    "\n",
    "def stem_words(words):\n",
    "    '''\n",
    "    Input\n",
    "    words: A list of words to be stemmed\n",
    "    Output\n",
    "    returns a list of stemmed words\n",
    "    '''\n",
    "    stemmer = PorterStemmer()\n",
    "    return [stemmer.stem(word) for word in words]\n",
    "\n",
    "def lemmatize_words(words):\n",
    "    '''\n",
    "    Input\n",
    "    words: A list of words to be lemmatized\n",
    "    Output\n",
    "    returns a list of lemmatized words\n",
    "    '''\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    return [lemmatizer.lemmatize(word) for word in words]\n",
    "\n",
    "def clean_data(content):\n",
    "\n",
    "\n",
    "    '''\n",
    "    Cleans the incoming data\n",
    "    Input\n",
    "    content: a DataFrame series\n",
    "    '''\n",
    "    cleaned_data = []\n",
    "    for i in range(len(content)):\n",
    "        #by iterating, we are using every row in clean_tweet as a string and we pass it to the cleaning functions\n",
    "        tweet = remove_unwanted_text(content[i].lower())\n",
    "        tb = TextBlob(tweet) #Tokenize the tweet after removing unwanted text\n",
    "        #tb is a textblob object, we use tb.words to access the tokenized words in it as a list of words\n",
    "        words = remove_punctuations(tb.words)\n",
    "        # stemmed_words = stem_words(words)  # Stem the words\n",
    "        lemmatized_words = lemmatize_words(words)\n",
    "        final_words = remove_stop_words(lemmatized_words)\n",
    "        cleaned_data.append(\" \".join(final_words)) #join the words as a string and append it\n",
    "    return cleaned_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "XxcgGGsVW3RN",
    "outputId": "a928c090-ac64-421e-a0bc-1f4d8da97ceb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ppl talk bad ghettohood kid growing nigga funnnnnnn'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Clean the 'tweet' column and add it in a new row \"clean_tweet\"\n",
    "df['clean_tweet'] = clean_data(df['tweet'])\n",
    "df['clean_tweet'][9]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6eV0EuZVdnQi"
   },
   "source": [
    "### **Converting to a hugging face dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "dYrIxGlBeF0Z"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 235
    },
    "id": "QuKZnD5MdugC",
    "outputId": "2bd8951e-ab8d-4014-887e-d537e8d9b4db"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['hate_speech_count', 'tweet', 'clean_tweet'],\n",
       "    num_rows: 10046\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "dataset = Dataset.from_pandas(df)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "9LQO915PloZK"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'hate_speech_count': 1,\n",
       " 'tweet': '\" @rhythmixx_ :hobbies include: fighting Mariam\"\\n\\nbitch',\n",
       " 'clean_tweet': 'hobby include fighting mariam bitch'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UJdGuKiSlcW5"
   },
   "source": [
    "### **Tokenizing the dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "hsaEulVDglsA"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3adc16b5183d4019baa70b4713656cdc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading config.json:   0%|          | 0.00/929 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84d7b4004ffc4517a66f7b6891d51cc5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9423e8f55edd4afca4c93ac4b223178c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3aa155f408c4fe28150ff64fedc5670",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (â€¦)cial_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "model_name = \"cardiffnlp/twitter-roberta-base-sentiment-latest\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "6uditTzBmA6f"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10046 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    }
   ],
   "source": [
    "def tokenize_function(examples):\n",
    "    tokenized_inputs = tokenizer(examples[\"clean_tweet\"], padding=True, truncation=True, return_tensors=\"pt\")\n",
    "    tokenized_inputs[\"label\"] = examples[\"hate_speech_count\"]\n",
    "    return tokenized_inputs\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "g7X6Ej3Fn1rv"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'hate_speech_count': 1,\n",
       " 'tweet': '\" @rhythmixx_ :hobbies include: fighting Mariam\"\\n\\nbitch',\n",
       " 'clean_tweet': 'hobby include fighting mariam bitch',\n",
       " 'input_ids': [0,\n",
       "  298,\n",
       "  27825,\n",
       "  680,\n",
       "  2190,\n",
       "  4401,\n",
       "  6009,\n",
       "  32594,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1],\n",
       " 'attention_mask': [1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " 'label': 1}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_dataset[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MJ6Mkf8Vu0ax",
    "tags": []
   },
   "source": [
    "### **Splitting the dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "1gXAhUFQu5hM"
   },
   "outputs": [],
   "source": [
    "train_val_split = tokenized_dataset.train_test_split(test_size=0.2)\n",
    "test_dataset = train_val_split['test']\n",
    "train_val_dataset = train_val_split['train']\n",
    "\n",
    "# Further split the train_val_dataset into train and validation sets\n",
    "train_val_split = train_val_dataset.train_test_split(test_size=0.25)\n",
    "train_dataset = train_val_split['train']\n",
    "val_dataset = train_val_split['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "eekPbve9_-1X"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset length: 6027\n",
      "Validation dataset length: 2009\n",
      "Test dataset length: 2010\n"
     ]
    }
   ],
   "source": [
    "# Print to verify splits\n",
    "print(f\"Train dataset length: {len(train_dataset)}\")\n",
    "print(f\"Validation dataset length: {len(val_dataset)}\")\n",
    "print(f\"Test dataset length: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M8TbwC0pHLAW",
    "tags": []
   },
   "source": [
    "### **Loading the model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "WmNQrmPwHN7n"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest and are newly initialized because the shapes did not match:\n",
      "- classifier.out_proj.weight: found shape torch.Size([3, 768]) in the checkpoint and torch.Size([2, 768]) in the model instantiated\n",
      "- classifier.out_proj.bias: found shape torch.Size([3]) in the checkpoint and torch.Size([2]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name,num_labels=2, ignore_mismatched_sizes=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wsNbkC_tIclW"
   },
   "source": [
    "### **Training the model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "B-OGH-c-IfOb"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/.local/lib/python3.7/site-packages/transformers/optimization.py:415: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments, DataCollatorWithPadding, AdamW\n",
    "import torch\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5, weight_decay=0.01)\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "for name, module in model.named_modules():\n",
    "    if isinstance(module, torch.nn.Dropout):\n",
    "        module.p = 0.3\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate= 2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=14,\n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model='precision',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "def compute_metrics(p):\n",
    "    preds = p.predictions.argmax(-1)\n",
    "    labels = p.label_ids\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average=\"weighted\")\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'precision': precision,\n",
    "        'recall':recall,\n",
    "        'f1': f1,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,  \n",
    "    compute_metrics=compute_metrics,\n",
    "    optimizers=(optimizer,None),  \n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5278' max='5278' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5278/5278 17:22, Epoch 14/14]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.635438</td>\n",
       "      <td>0.742160</td>\n",
       "      <td>0.759635</td>\n",
       "      <td>0.742160</td>\n",
       "      <td>0.738497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.569200</td>\n",
       "      <td>0.509100</td>\n",
       "      <td>0.769537</td>\n",
       "      <td>0.772642</td>\n",
       "      <td>0.769537</td>\n",
       "      <td>0.769133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.487400</td>\n",
       "      <td>0.478878</td>\n",
       "      <td>0.792434</td>\n",
       "      <td>0.792926</td>\n",
       "      <td>0.792434</td>\n",
       "      <td>0.792245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.461500</td>\n",
       "      <td>0.504087</td>\n",
       "      <td>0.792434</td>\n",
       "      <td>0.792524</td>\n",
       "      <td>0.792434</td>\n",
       "      <td>0.792446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.461500</td>\n",
       "      <td>0.490903</td>\n",
       "      <td>0.797909</td>\n",
       "      <td>0.800254</td>\n",
       "      <td>0.797909</td>\n",
       "      <td>0.797319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.436900</td>\n",
       "      <td>0.525475</td>\n",
       "      <td>0.794425</td>\n",
       "      <td>0.794787</td>\n",
       "      <td>0.794425</td>\n",
       "      <td>0.794275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.421200</td>\n",
       "      <td>0.532209</td>\n",
       "      <td>0.784968</td>\n",
       "      <td>0.788267</td>\n",
       "      <td>0.784968</td>\n",
       "      <td>0.784581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.509320</td>\n",
       "      <td>0.792932</td>\n",
       "      <td>0.792955</td>\n",
       "      <td>0.792932</td>\n",
       "      <td>0.792893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.488849</td>\n",
       "      <td>0.794425</td>\n",
       "      <td>0.797596</td>\n",
       "      <td>0.794425</td>\n",
       "      <td>0.793644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.386900</td>\n",
       "      <td>0.497639</td>\n",
       "      <td>0.800398</td>\n",
       "      <td>0.800693</td>\n",
       "      <td>0.800398</td>\n",
       "      <td>0.800274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.377100</td>\n",
       "      <td>0.536062</td>\n",
       "      <td>0.800398</td>\n",
       "      <td>0.800485</td>\n",
       "      <td>0.800398</td>\n",
       "      <td>0.800337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.362500</td>\n",
       "      <td>0.489988</td>\n",
       "      <td>0.799900</td>\n",
       "      <td>0.799941</td>\n",
       "      <td>0.799900</td>\n",
       "      <td>0.799857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.362500</td>\n",
       "      <td>0.508557</td>\n",
       "      <td>0.802887</td>\n",
       "      <td>0.803011</td>\n",
       "      <td>0.802887</td>\n",
       "      <td>0.802815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.357400</td>\n",
       "      <td>0.511673</td>\n",
       "      <td>0.803385</td>\n",
       "      <td>0.803581</td>\n",
       "      <td>0.803385</td>\n",
       "      <td>0.803291</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=5278, training_loss=0.4223957791749435, metrics={'train_runtime': 1042.5865, 'train_samples_per_second': 80.931, 'train_steps_per_second': 5.062, 'total_flos': 2287501904185680.0, 'train_loss': 0.4223957791749435, 'epoch': 14.0})"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='252' max='126' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [126/126 00:11]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Results: {'eval_loss': 0.8049112558364868, 'eval_accuracy': 0.8242906918865107, 'eval_precision': 0.8251956179511837, 'eval_recall': 0.8242906918865107, 'eval_f1': 0.8242522912568999, 'eval_runtime': 5.6822, 'eval_samples_per_second': 353.56, 'eval_steps_per_second': 22.174, 'epoch': 5.0}\n",
      "Test Results: {'eval_loss': 0.8430566787719727, 'eval_accuracy': 0.8393034825870647, 'eval_precision': 0.8398028620336193, 'eval_recall': 0.8393034825870647, 'eval_f1': 0.8392367487313183, 'eval_runtime': 5.7312, 'eval_samples_per_second': 350.712, 'eval_steps_per_second': 21.985, 'epoch': 5.0}\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the validation dataset\n",
    "evaluation_results = trainer.evaluate(eval_dataset=val_dataset)\n",
    "print(\"Validation Results:\", evaluation_results)\n",
    "\n",
    "# Optionally, evaluate the model on the test dataset\n",
    "test_results = trainer.evaluate(eval_dataset=test_dataset)\n",
    "print(\"Test Results:\", test_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./trained_roBERTa_model/tokenizer_config.json',\n",
       " './trained_roBERTa_model/special_tokens_map.json',\n",
       " './trained_roBERTa_model/vocab.json',\n",
       " './trained_roBERTa_model/merges.txt',\n",
       " './trained_roBERTa_model/added_tokens.json',\n",
       " './trained_roBERTa_model/tokenizer.json')"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the trained model\n",
    "model.save_pretrained('./trained_roBERTa_model')\n",
    "tokenizer.save_pretrained('./trained_roBERTa_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class: 1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load the saved tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('./trained_roBERTa_model')\n",
    "\n",
    "# Load the saved model\n",
    "model = AutoModelForSequenceClassification.from_pretrained('./trained_roBERTa_model')\n",
    "\n",
    "# Example usage: Tokenize a new input and make a prediction\n",
    "inputs = tokenizer(\"This is very hateful.\", return_tensors=\"pt\")\n",
    "outputs = model(**inputs)\n",
    "predictions = outputs.logits.argmax(-1)\n",
    "print(\"Predicted class:\", predictions.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory ./results has been deleted successfully.\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "\n",
    "# Path to the directory you want to delete\n",
    "directory_path = './results'\n",
    "\n",
    "# Delete the directory\n",
    "try:\n",
    "    shutil.rmtree(directory_path)\n",
    "    print(f'Directory {directory_path} has been deleted successfully.')\n",
    "except FileNotFoundError:\n",
    "    print(f'Directory {directory_path} does not exist.')\n",
    "except Exception as e:\n",
    "    print(f'An error occurred: {e}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "k6lKjESWQxmu",
    "hfCaRsV4Wl3x",
    "6eV0EuZVdnQi"
   ],
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
